# -*- coding: utf-8 -*-
"""data_analysis_using_Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QUJoPJpk79bpMc3PMvmjzeR1DLmlKflM
"""

# --- Cell: 1) Setup Spark and imports ---
from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.window import Window
import matplotlib.pyplot as plt
import numpy as np
import os

# Start Spark (local[*] is fine for the notebook)
spark = SparkSession.builder \
    .appName("Employees-EDA") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()

# Optional: speedup conversion to pandas if pyarrow is available
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

print("Spark version:", spark.version)

"""2. UPLOAD CSV.

"""

from google.colab import files
uploaded = files.upload()

"""3. Define Schema & Load Dataset

- Define an explicit schema for employee data (Emp_No, Emp_Name, Salary, Age, Department).
- Read the CSV file using Spark with header = True.
- Clean whitespace from text fields (Emp_Name, Department).
- Cache the DataFrame for reuse.
- Print schema and show sample rows.

"""

# --- Cell: 2) Read CSV with explicit schema (safer than inferring) ---
schema = T.StructType([
    T.StructField("Emp_No", T.IntegerType(), True),
    T.StructField("Emp_Name", T.StringType(), True),
    T.StructField("Salary", T.IntegerType(), True),
    T.StructField("Age", T.IntegerType(), True),
    T.StructField("Department", T.StringType(), True)
])

df = spark.read.csv("employees.csv", header=True, schema=schema, mode="DROPMALFORMED")
# trim whitespace on string columns
df = df.withColumn("Emp_Name", F.trim(F.col("Emp_Name"))) \
       .withColumn("Department", F.trim(F.col("Department")))

df = df.cache()   # cache since we will reuse it
print("Rows:", df.count())
df.printSchema()
df.show(10, truncate=False)

"""4. Generate Summary Statistics

- Use describe() to compute summary statistics (count, mean, stddev, min, max) for numeric columns.

"""

# --- Cell: 3) Basic EDA ---
print("Summary (describe):")
df.describe().show()

# Null / blank counts per column
null_counts = df.select([
    F.count(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == ""), c)).alias(c)
    for c in df.columns
])
print("Null / blank counts:")
null_counts.show()

# Distinct counts
print("Distinct counts:")
df.select([F.countDistinct(c).alias(c) for c in df.columns]).show()

# Quick value counts for Department
print("Department counts:")
df.groupBy("Department").count().orderBy(F.desc("count")).show(truncate=False)

"""5. Department-Wise Statistics

- Group data by Department.
- Calculate average salary, average age, and employee count per department.
- Show results.

"""

# --- Cell: 4) Cleaning and simple outlier removal ---
# drop exact duplicates
df = df.dropDuplicates()

# remove impossible values
df = df.filter((F.col("Salary") > 0) & (F.col("Age") > 0) & (F.col("Age") < 100))

# IQR outlier filter for Salary (optional)
q1, q3 = df.approxQuantile("Salary", [0.25, 0.75], 0.01)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
print(f"Salary IQR bounds: {lower} .. {upper}")

df = df.filter((F.col("Salary") >= lower) & (F.col("Salary") <= upper))

print("Rows after cleaning:", df.count())

"""6. Find Highest and Lowest Salaries

- Sort dataset by Salary in descending order to get top 5 highest salaries.
- Sort dataset by Salary in ascending order to get bottom 5 lowest salaries.

"""

# --- Cell: 5) Feature engineering ---
df = df.withColumn("Salary_k", (F.col("Salary") / 1000).cast("double")) \
       .withColumn("Salary_Band",
           F.when(F.col("Salary") < 5000, "<5k")
            .when(F.col("Salary") < 10000, "5-10k")
            .when(F.col("Salary") < 20000, "10-20k")
            .otherwise(">20k")
       ) \
       .withColumn("Age_Group",
           F.when(F.col("Age") < 30, "<30")
            .when(F.col("Age") < 40, "30-39")
            .otherwise("40+")
       )

df.select("Emp_No","Emp_Name","Department","Salary","Salary_Band","Age","Age_Group").show(10, truncate=False)

"""7. Salary Distribution

- Categorize employees into buckets based on salary:
   • Low (<30K)
   • Medium (30K–60K)
   • High (≥60K)
- Count number of employees in each salary bucket.

"""

# --- Cell: 6) Aggregations per department (avg, median, std, min, max) ---
dept_stats = df.groupBy("Department").agg(
    F.count("*").alias("count"),
    F.round(F.avg("Salary"),2).alias("avg_salary"),
    F.expr("percentile_approx(Salary, 0.5)").alias("median_salary"),
    F.max("Salary").alias("max_salary"),
    F.min("Salary").alias("min_salary"),
    F.round(F.stddev("Salary"),2).alias("std_salary")
).orderBy(F.desc("avg_salary"))

dept_stats.show(truncate=False)

"""8. Employees Above Average Salary

- Calculate the overall average salary.
- Filter employees whose Salary is greater than the average.
- Display top results.

"""

# --- Cell: 7) Top earners & window functions ---
print("Top 10 earners overall:")
df.orderBy(F.desc("Salary")).select("Emp_No","Emp_Name","Department","Salary","Age").limit(10).show(truncate=False)

w = Window.partitionBy("Department").orderBy(F.desc("Salary"))
top3_dept = df.withColumn("dept_rank", F.row_number().over(w)).filter(F.col("dept_rank") <= 3)
print("Top 3 earners per department:")
top3_dept.select("Department","dept_rank","Emp_No","Emp_Name","Salary").orderBy("Department","dept_rank").show(200, truncate=False)

"""9. Add 10% Bonus to Salaries

- Create a new column "SalaryAfterBonus".
- Calculate new salary by adding 10% bonus to existing salary:
   SalaryAfterBonus = Salary * 1.10
- Show updated DataFrame.

"""

# --- Cell: 8) Correlation ---
corr_age_salary = df.stat.corr("Age", "Salary")
print(f"Pearson correlation between Age and Salary: {corr_age_salary:.3f}")

"""10. Top 3 Salaries per Department

- Use Window functions to rank employees by Salary within each Department.
- Select only top 3 employees in each department.
- Show results.

"""

# --- Cell: 9) Visualizations (matplotlib) ---
# convert to pandas for plotting (sample if dataset is large)
n = df.count()
if n > 20000:
    frac = 20000.0 / n
    print(f"Sampling {frac:.4f} of data (~20k rows) for plotting")
    plot_pdf = df.sample(False, frac, seed=42).toPandas()
else:
    plot_pdf = df.toPandas()

import matplotlib.pyplot as plt
plt.rcParams.update({'figure.max_open_warning': 0})

# Salary histogram
plt.figure(figsize=(8,5))
plt.hist(plot_pdf['Salary'].dropna(), bins=12)
plt.title('Salary distribution')
plt.xlabel('Salary')
plt.ylabel('Count')
plt.grid(axis='y', alpha=0.6)
plt.show()

# Salary boxplot
plt.figure(figsize=(6,3))
plt.boxplot(plot_pdf['Salary'].dropna(), vert=False)
plt.title('Salary boxplot')
plt.xlabel('Salary')
plt.show()

# Employees per department (bar)
plt.figure(figsize=(8,4))
plot_pdf['Department'].value_counts().sort_values(ascending=False).plot(kind='bar')
plt.title('Employees per Department')
plt.xlabel('Department')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Average salary per department (bar)
plt.figure(figsize=(8,4))
plot_pdf.groupby('Department')['Salary'].mean().sort_values(ascending=False).plot(kind='bar')
plt.title('Average Salary by Department')
plt.ylabel('Avg Salary')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Scatter Salary vs Age + linear fit
plt.figure(figsize=(7,5))
x = plot_pdf['Age'].dropna()
y = plot_pdf['Salary'].dropna()
plt.scatter(x, y, alpha=0.7)
if len(x) > 1:
    z = np.polyfit(x, y, 1)
    p = np.poly1d(z)
    xs = np.linspace(x.min(), x.max(), 100)
    plt.plot(xs, p(xs), linestyle='--', linewidth=2)
plt.title('Salary vs Age')
plt.xlabel('Age')
plt.ylabel('Salary')
plt.show()

"""11. Age Distribution

- Categorize employees into age groups:
   • Young (<30)
   • Mid (30–49)
   • Senior (50+)
- Count employees in each age group.

"""

# --- Cell: 10) Save outputs ---
# create output dir
os.makedirs("output", exist_ok=True)

# save department stats (as CSV)
dept_stats.coalesce(1).write.mode("overwrite").option("header", True).csv("output/department_stats_csv")

# save cleaned dataset as parquet
df.write.mode("overwrite").parquet("output/clean_employees.parquet")

print("Saved: output/department_stats_csv/  and  output/clean_employees.parquet")

"""12. Export Processed Data

- Write the cleaned DataFrame to CSV format.
- Save output to a folder "processed_employees" with header = True.

"""

# --- Cell: 11) Performance tips example ---
# Repartition by department before heavy aggregations (if dataset is large)
df_repart = df.repartition(4, "Department")
df_repart.groupBy("Department").agg(F.avg("Salary")).show()

# If joining a small lookup table (e.g., dept metadata), prefer broadcast join:
# from pyspark.sql import functions as F
# small_df = spark.createDataFrame([...], schema_small)
# joined = df.join(F.broadcast(small_df), "Department")

"""14. Stop Spark Session

- Stop the Spark session to release resources.

"""

# --- Cell: 12) Summary & insight printout ---
print("==== Quick summary ====")
print("Total employees analyzed:", df.count())
print("Correlation (Age vs Salary):", round(corr_age_salary, 3))
print("Top departments by average salary:")
dept_stats.show(truncate=False)